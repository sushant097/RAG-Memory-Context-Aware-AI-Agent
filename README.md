# ğŸ§  RAG with Memory â€” Agentic Web Memory Backend

A fully-agentic **AI memory system** built with `FastAPI`, `Gemini`, and `FAISS`.
It continuously **learns from the web pages you visit**, builds a **semantic memory** of them, and later **recalls the precise snippet and source** when we ask â€” functioning as our **personal, retrieval-augmented web memory agent**.

---

## ğŸš€ Concept

This project turns the classic RAG (Retrieval-Augmented Generation) pipeline into a **living memory system** â€” one that perceives, decides, acts, and remembers.

> â€œIt doesnâ€™t just search â€” it *remembers* what we read, when we read it, and where we saw it.â€

We can think of it as a self-contained *AI hippocampus* for the browser.

---

## ğŸ§© Agentic Architecture

The backend follows the cognitive architecture:

> **Agent â†’ Perception â†’ Memory â†’ Decision â†’ Action**

Each layer mirrors a mental function, powered by **Gemini models** and **vector memory**.

```
Chrome Extension
     â†“
  HTTP API (FastAPI)
     â†“
   Core Logic
     â†“
  MCP Tools (index/search)
     â†“
  Agentic Loop (Gemini reasoning)
     â†“
  FAISS Vector Store (long-term memory)
```

| Layer          | Role                                                                                      |             |           |
| -------------- | ----------------------------------------------------------------------------------------- | ----------- | --------- |
| **Perception** | Gemini interprets text/query, classifies intent, and hints which tool to use.             |             |           |
| **Decision**   | Gemini planner outputs structured calls like `FUNCTION_CALL: search_documents             | query="..." | top_k=5`. |
| **Action**     | Executes the function, then writes the result back into short-term memory.                |             |           |
| **Memory**     | Maintains both *working memory* (session context) and *long-term FAISS memory*.           |             |           |
| **MCP Tools**  | Provide clean modular interfaces (`index_page`, `search_documents`, `process_documents`). |             |           |

---

## ğŸ”¬ Key Idea

Each webpage is broken into **semantic chunks**, embedded via **Googleâ€™s `text-embedding-004`** (or optionally local Nomic embeddings), and stored in a FAISS vector store.
When a user later asks a question, the system performs **semantic + temporal retrieval**, boosting newer content and returning the *exact snippet* and *URL* where it appeared.

---

## ğŸ’¡ Example Scenario

1. We read multiple pages on *vector databases*.
2. Weeks later if we ask:

   > â€œWhich article explained IVF and HNSW in FAISS?â€
3. The agent searches its memory and returns:

   ```
   â€œIVF and HNSW indexing accelerate large-scale similarity search...â€
   [Source: https://example.com/vector-db, ID: a3c1_002]
   ```

   â†’ The extension opens the page and highlights that text.

---

## ğŸ§  Core Features

| Feature                       | Description                                                                          |
| ----------------------------- | ------------------------------------------------------------------------------------ |
| **Gemini-Driven Reasoning**   | Both perception and decision use Gemini 2.0 Flash for intelligent planning.          |
| **Dual Embedding Backend**    | Supports *Google embeddings* for precision or *Ollama/Nomic* for offline use.        |
| **Temporal Awareness**        | Adds time-decay weighting: recent knowledge ranks higher in retrieval.               |
| **Deduplication**             | SHA-1 hashing avoids re-embedding duplicate content.                                 |
| **Hybrid Memory**             | Short-term (RAM) + long-term (FAISS) = contextual continuity.                        |
| **MCP + REST Dual Interface** | Accessible both as an MCP stdio toolset and as a FastAPI HTTP service.               |
| **Document Ingestion**        | Converts `.html`, `.pdf`, `.docx`, `.md` via MarkItDown for batch indexing.          |
| **Extension-Ready**           | `/index_page` and `/search` endpoints integrate directly with Chrome MV3 extensions. |

---

## âš™ï¸ Tech Stack

| Layer          | Technology                                              | Purpose                                     |
| -------------- | ------------------------------------------------------- | ------------------------------------------- |
| **LLM**        | Gemini 2.0 Flash                                        | Perception & decision reasoning             |
| **Embeddings** | Google `text-embedding-004` / Ollama `nomic-embed-text` | Vector representations                      |
| **Vector DB**  | FAISS                                                   | Nearest-neighbor retrieval                  |
| **Protocol**   | MCP (Model Context Protocol)                            | Modular tool calls                          |
| **API**        | FastAPI                                                 | Bridge for Chrome extension                 |
| **Parsing**    | MarkItDown                                              | Clean text extraction from HTML/PDF         |
| **Package**    | uv                                                      | Dependency management & virtual environment |

---

## ğŸ§® Data & Memory Model

| Type                   | Description                                                       |
| ---------------------- | ----------------------------------------------------------------- |
| **Short-Term Memory**  | Session-scoped objects managed in RAM (`memory.py`).              |
| **Long-Term Memory**   | FAISS index + JSONL metadata with embeddings, titles, timestamps. |
| **Temporal Weighting** | `score = sim * (1 + Î± * freshness(days))` â€” prioritizes recency.  |
| **Metadata Schema**    | `{url, title, snippet, chunk_id, timestamp, score}`               |
| **Chunking**           | ~900 characters with 160-char overlap for semantic continuity.    |

---

## ğŸ“¦ Repository Structure

```
rag_memory_agent/
â”œâ”€â”€ core.py           # Core FAISS, embedding, chunking, and indexing logic
â”œâ”€â”€ mcp_tools.py      # MCP-decorated tools (index_page, search_documents)
â”œâ”€â”€ http.py           # REST endpoints for Chrome extension
â”œâ”€â”€ agent.py          # Orchestrator for Gemini reasoning loop
â”œâ”€â”€ perception.py     # Gemini perception (intent extraction)
â”œâ”€â”€ decision.py       # Gemini decision (planner)
â”œâ”€â”€ action.py         # Executes tool calls
â”œâ”€â”€ memory.py         # Short-term session memory
â”œâ”€â”€ models.py         # Pydantic schemas
â”œâ”€â”€ config.py         # Centralized configuration (.env loader)
â”œâ”€â”€ documents/        # Optional batch ingestion folder
â””â”€â”€ faiss_index/      # Persistent FAISS store + metadata.jsonl
```


---

## ğŸ§­ Data Flow

### ğŸ”¹ Indexing

```
Chrome â†’ POST /index_page
     â†“
FastAPI â†’ core.index_page_core()
     â†“
Chunks â†’ Embeddings â†’ FAISS + metadata
```

### ğŸ”¹ Searching

```
User query â†’ perception.py (Gemini)
     â†“
decision.py â†’ FUNCTION_CALL: search_documents
     â†“
action.py â†’ core.search_documents_core()
     â†“
FAISS search (semantic + temporal)
     â†“
Return URLs + snippets â†’ highlight in Chrome
```

---

## ğŸ—ï¸ Running the Backend

### 1ï¸âƒ£ Environment & Install

```bash
uv venv
uv sync
```


### 2ï¸âƒ£ Choose Embedding Provider

You can run the system with **either** local Ollama or cloud-based Google embeddings.

#### ğŸ§© Option A â€” Local (Ollama)

1. **Download Ollama**
   â†’ [https://ollama.com/download](https://ollama.com/download)

2. **Pull and run the model**

   ```bash
   ollama pull nomic-embed-text
   ollama serve
   ```

3. **Set provider in `.env`**

   ```bash
   EMBEDDINGS_PROVIDER=ollama
   EMBED_URL=http://localhost:11434/api/embeddings
   EMBED_MODEL=nomic-embed-text
   ```

#### â˜ï¸ Option B â€” Google Embeddings

1. **Set provider and credentials in `.env`:**

   ```bash
   # "google" or "ollama"
   EMBEDDINGS_PROVIDER=google
   GOOGLE_API_KEY="<insert_your_api_key>"
   GOOGLE_EMBED_MODEL=text-embedding-004
   ```

2. Ensure dependencies are installed:

   ```bash
   uv add llama-index-embeddings-google-genai google-genai
   ```

---

### 3ï¸âƒ£ Start the API

```bash
uvicorn rag_memory_agent.http:app --reload --port 8000
```

---

### 4ï¸âƒ£ Index & Search

```bash
curl -X POST http://localhost:8000/index_page \
  -H "content-type: application/json" \
  -d '{"url":"https://example.com","title":"Example","text":"Vector DBs use IVF and HNSW..."}'

curl "http://localhost:8000/search?q=vector%20dbs"
```

---
### 5ï¸âƒ£ CLI Agent

```bash
python -m rag_memory_agent.agent
> what was that blog about HNSW indexing?
```

---

## ğŸ§© Loading the Chrome Extension

The repository includes a working Chrome MV3 extension inside the **`extension/`** folder.  
It provides a minimal UI to search your indexed pages and highlights the retrieved snippet directly on the original website.

### ğŸ”¹ Steps

1. **Open Chrome â†’ `chrome://extensions`**
2. **Enable Developer mode** (top-right toggle)
3. **Click â€œLoad unpackedâ€** and select the projectâ€™s `extension/` folder.
4. Pin ğŸ§  **Web Memory** from the extensions bar.

### ğŸ”¹ Usage

- The extension automatically indexes pages you visit (except confidential ones like Gmail, Slack, WhatsApp, etc.).  
- Click the extension icon to **search your web memory**.  
- Select a result â†’ the page opens and **highlights** the matching text.  
- You can configure the backend URL and denylist in the extensionâ€™s **Options page**.

> Make sure the backend FastAPI server is running at `http://localhost:8000` (or the URL you set in Options).



## ğŸ§© MCP Tools

| Tool                  | Description                                     |
| --------------------- | ----------------------------------------------- |
| **index_page**        | Ingests live web text (chunks â†’ embed â†’ FAISS). |
| **search_documents**  | Returns semantic matches with source metadata.  |
| **process_documents** | Batch-ingests `/documents` folder.              |

---

## ğŸ† Unique Aspects

âœ… **Unified Core Architecture** : All indexing, retrieval, and embedding logic consolidated in `core.py`, ensuring MCP, REST, and agent all share one codebase.

âœ… **Temporal & Semantic Hybrid Ranking**: Combines cosine similarity with a lightweight temporal decay model â€” newer memories surface first.

```python
  score = sim * (1 + Î± * freshness(days))
  # newer pages rank higher.
  ```

âœ… **Dual-Mode Memory**: Supports **short-term** (RAM) and **long-term** (FAISS) memory separation â€” enabling hybrid reasoning loops.

âœ… **Dual Transport (MCP + REST)**: Works both as a traditional MCP stdio toolset *and* a REST API â€” bridging AI agent ecosystems and browser extensions.

âœ… **Dynamic Embedding Backend**: Can seamlessly switch between local (`ollama nomic-embed-text`) and cloud (`text-embedding-004`) without touching code.

âœ… **Agentic Reasoning with Gemini**: Perception and decision stages leverage Gemini 2.0 Flash for contextual tool planning, not static prompts.

âœ… **Data Efficiency**: Uses deduplicated SHA1 chunk hashing and JSONL metadata for minimal storage overhead.

âœ… **Practical RAG Evolution**: Instead of ephemeral chat memory, this agent builds a persistent semantic map of what the user reads online.

---

## ğŸŒ Online / Offline Mode

The backend dynamically adapts between **Gemini-powered reasoning** and a **fully local fallback** depending on environment configuration.

* When `GEMINI_API_KEY` (or `GOOGLE_API_KEY`) is **available** in `.env`, both **Perception** and **Decision** modules use **Gemini 2.0 Flash** for natural-language understanding and planning.
* When no key is present, they **gracefully fall back** to lightweight, rule-based logic â€” still able to perform indexing and semantic retrieval locally.
* All vector embeddings, FAISS indexing, and search remain identical in either mode.

```text
Online  â†’ Gemini reasoning + Google or Ollama embeddings
Offline â†’ Rule-based reasoning + Ollama embeddings (no internet required)
```

This ensures the system works seamlessly **offline with Ollama** or **online with Gemini/Google**, without any code changes â€” only `.env` variables determine the mode.

---

## ğŸ•’ Temporal & Semantic Hybrid Ranking

Retrieval now blends **semantic similarity** with a **bounded temporal-popularity signal** instead of multiplying them.
Each FAISS hit is scored as a *weighted sum* where semantics dominate and freshness / frequency provide a gentle nudge.

### Concept

```python
# Weighted-blend scoring (Option A)
score = (SIM_WEIGHT * sim) + (TEMP_WEIGHT * hybrid)

# where:
#   hybrid = w_f * freshness + w_p * popularity
#   freshness = exp(-Î» * days)      # exponential half-life (â‰ˆ7 days)
#   popularity = 1 - exp(-visits/3) # saturating with repeated visits
```

**How it works**

| Component        | Meaning                                                                                                            |
| ---------------- | ------------------------------------------------------------------------------------------------------------------ |
| **sim**          | Pure cosine similarity between the query and stored chunk.                                                         |
| **freshness**    | Exponentially decays with age so recent pages retain higher influence.                                             |
| **popularity**   | Increases smoothly with visit count; a few revisits help, excessive visits saturate.                               |
| **hybrid**       | Linear blend of freshness (`w_f`) and popularity (`w_p`).                                                          |
| **Weighted sum** | Final `score` is mostly semantic (`SIM_WEIGHT â‰ˆ 0.9`) with a small temporal-popularity term (`TEMP_WEIGHT â‰ˆ 0.1`). |

This keeps **semantic relevance as the main driver** while allowing *recent or repeatedly visited pages* to break ties among equally similar results.

### Default parameters

| Parameter                                | Default   | Effect                            |
| ---------------------------------------- | --------- | --------------------------------- |
| `HALF_LIFE_DAYS`                         | 7         | Freshness halves every 7 days     |
| `SIM_WEIGHT` / `TEMP_WEIGHT`             | 0.9 / 0.1 | Semantics â‰ˆ 90 %, Temporal â‰ˆ 10 % |
| `FRESHNESS_WEIGHT` / `POPULARITY_WEIGHT` | 0.7 / 0.3 | Blend between recency & visits    |
| `RECENCY_ALPHA`                          | â€”         | (Deprecated in blend mode)        |

### Intuition

> *Semantic similarity decides whatâ€™s relevant;
> temporal-popularity decides which relevant results feel freshest.*

In practice, this means that if two pages explain the same concept equally well, the one we **read recently** or **visit more often** will appear first â€” while irrelevant pages never outrank semantically strong ones.

---

## Summary

This project demonstrates how **RAG can evolve into long-term memory**:
an AI system that learns continuously, remembers semantically, and retrieves with context awareness â€” bridging *information retrieval*, *memory persistence*, and *agentic cognition*.
