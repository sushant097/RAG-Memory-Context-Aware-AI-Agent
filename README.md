# ðŸ§  RAG with Memory â€” Agentic Web Memory Backend

A fully-agentic **AI memory system** built with `FastAPI`, `Gemini`, and `FAISS`.
It continuously **learns from the web pages you visit**, builds a **semantic memory** of them, and later **recalls the precise snippet and source** when you ask â€” functioning as your **personal, retrieval-augmented web memory agent**.

---

## ðŸš€ Concept

This project turns the classic RAG (Retrieval-Augmented Generation) pipeline into a **living memory system** â€” one that perceives, decides, acts, and remembers.

> â€œIt doesnâ€™t just search â€” it *remembers* what you read, when you read it, and where you saw it.â€

You can think of it as a self-contained *AI hippocampus* for your browser.

---

## ðŸ§© Agentic Architecture

The backend follows the cognitive architecture:

> **Agent â†’ Perception â†’ Memory â†’ Decision â†’ Action**

Each layer mirrors a mental function, powered by **Gemini models** and **vector memory**.

```
Chrome Extension
     â†“
  HTTP API (FastAPI)
     â†“
   Core Logic
     â†“
  MCP Tools (index/search)
     â†“
  Agentic Loop (Gemini reasoning)
     â†“
  FAISS Vector Store (long-term memory)
```

| Layer          | Role                                                                                      |             |           |
| -------------- | ----------------------------------------------------------------------------------------- | ----------- | --------- |
| **Perception** | Gemini interprets text/query, classifies intent, and hints which tool to use.             |             |           |
| **Decision**   | Gemini planner outputs structured calls like `FUNCTION_CALL: search_documents             | query="..." | top_k=5`. |
| **Action**     | Executes the function, then writes the result back into short-term memory.                |             |           |
| **Memory**     | Maintains both *working memory* (session context) and *long-term FAISS memory*.           |             |           |
| **MCP Tools**  | Provide clean modular interfaces (`index_page`, `search_documents`, `process_documents`). |             |           |

---

## ðŸ”¬ Key Idea

Each webpage is broken into **semantic chunks**, embedded via **Googleâ€™s `text-embedding-004`** (or optionally local Nomic embeddings), and stored in a FAISS vector store.
When a user later asks a question, the system performs **semantic + temporal retrieval**, boosting newer content and returning the *exact snippet* and *URL* where it appeared.

---

## ðŸ’¡ Example Scenario

1. You read multiple pages on *vector databases*.
2. Weeks later you ask:

   > â€œWhich article explained IVF and HNSW in FAISS?â€
3. The agent searches its memory and returns:

   ```
   â€œIVF and HNSW indexing accelerate large-scale similarity search...â€
   [Source: https://example.com/vector-db, ID: a3c1_002]
   ```

   â†’ The extension opens the page and highlights that text.

---

## ðŸ§  Core Features

| Feature                       | Description                                                                          |
| ----------------------------- | ------------------------------------------------------------------------------------ |
| **Gemini-Driven Reasoning**   | Both perception and decision use Gemini 2.0 Flash for intelligent planning.          |
| **Dual Embedding Backend**    | Supports *Google embeddings* for precision or *Ollama/Nomic* for offline use.        |
| **Temporal Awareness**        | Adds time-decay weighting: recent knowledge ranks higher in retrieval.               |
| **Deduplication**             | SHA-1 hashing avoids re-embedding duplicate content.                                 |
| **Hybrid Memory**             | Short-term (RAM) + long-term (FAISS) = contextual continuity.                        |
| **MCP + REST Dual Interface** | Accessible both as an MCP stdio toolset and as a FastAPI HTTP service.               |
| **Document Ingestion**        | Converts `.html`, `.pdf`, `.docx`, `.md` via MarkItDown for batch indexing.          |
| **Extension-Ready**           | `/index_page` and `/search` endpoints integrate directly with Chrome MV3 extensions. |

---

## âš™ï¸ Tech Stack

| Layer             | Technology                                 | Purpose                             |
| ----------------- | ------------------------------------------ | ----------------------------------- |
| **LLM**           | Gemini 2.0 Flash                           | Perception & decision reasoning     |
| **Embeddings**    | Google `text-embedding-004` / Ollama Nomic | Vector representations              |
| **Vector DB**     | FAISS                                      | Nearest-neighbor retrieval          |
| **Protocol**      | MCP (Model Context Protocol)               | Modular tool calls                  |
| **API**           | FastAPI                                    | Bridge for Chrome extension         |
| **Parsing**       | MarkItDown                                 | Clean text extraction from HTML/PDF |
| **Orchestration** | uv                                         | Modern Python dependency management |

---

## ðŸ§® Data & Memory Model

| Type                   | Description                                                       |
| ---------------------- | ----------------------------------------------------------------- |
| **Short-Term Memory**  | Session-scoped objects managed in RAM (`memory.py`).              |
| **Long-Term Memory**   | FAISS index + JSONL metadata with embeddings, titles, timestamps. |
| **Temporal Weighting** | `score = sim * (1 + Î± * freshness(days))` â€” prioritizes recency.  |
| **Metadata Schema**    | `{url, title, snippet, chunk_id, timestamp, score}`               |
| **Chunking**           | ~900 characters with 160-char overlap for semantic continuity.    |

---

## ðŸ“¦ Repository Structure

```
rag_memory_agent/
â”œâ”€â”€ core.py           # All FAISS, embedding, chunking, and indexing logic
â”œâ”€â”€ mcp_tools.py      # MCP-decorated tools (index_page, search_documents)
â”œâ”€â”€ http.py           # REST endpoints for Chrome extension
â”œâ”€â”€ agent.py          # Orchestrator for Gemini-powered reasoning loop
â”œâ”€â”€ perception.py     # Gemini perception layer (intent extraction)
â”œâ”€â”€ decision.py       # Gemini decision layer (planner)
â”œâ”€â”€ action.py         # Executes tool calls and manages output
â”œâ”€â”€ memory.py         # Short-term session memory
â”œâ”€â”€ models.py         # Config + Pydantic schemas
â”œâ”€â”€ documents/        # Optional folder for batch ingestion
â””â”€â”€ faiss_index/      # Persistent FAISS store + metadata.jsonl
```

---

## ðŸ§­ Data Flow

### ðŸ”¹ Indexing

```
Chrome â†’ POST /index_page
     â†“
FastAPI â†’ core.index_page_core()
     â†“
Chunks â†’ Embeddings â†’ FAISS + metadata
```

### ðŸ”¹ Searching

```
User query â†’ perception.py (Gemini)
     â†“
decision.py â†’ FUNCTION_CALL: search_documents
     â†“
action.py â†’ core.search_documents_core()
     â†“
FAISS search (semantic + temporal)
     â†“
Return URLs + snippets â†’ highlight in Chrome
```

---

## ðŸ—ï¸ Running the Backend

### 1ï¸âƒ£ Environment & Install

```bash
uv venv
uv sync
```

### 2ï¸âƒ£ Choose Embedding Provider

```bash
# A) Google (requires key)
export EMBEDDINGS_PROVIDER=google
export GOOGLE_API_KEY=your_key
# B) Local (Ollama)
ollama pull nomic-embed-text
ollama serve
```

### 3ï¸âƒ£ Start API

```bash
uvicorn rag_memory_agent.http:app --reload --port 8000
```

### 4ï¸âƒ£ Index & Search

```bash
curl -X POST http://localhost:8000/index_page \
  -H 'content-type: application/json' \
  -d '{"url":"https://example.com","title":"Example","text":"Vector DBs use IVF, HNSW..."}'

curl "http://localhost:8000/search?q=vector%20dbs"
```

### 5ï¸âƒ£ CLI Agent

```bash
python -m rag_memory_agent.agent
> what was that blog about HNSW indexing?
```

---

## ðŸ§© MCP Tools

| Tool                  | Description                                     |
| --------------------- | ----------------------------------------------- |
| **index_page**        | Ingests live web text (chunks â†’ embed â†’ FAISS). |
| **search_documents**  | Returns semantic matches with source metadata.  |
| **process_documents** | Batch-ingests `/documents` folder.              |

---

## ðŸ† Unique Aspects

âœ… **Unified Core Architecture**

All indexing, retrieval, and embedding logic consolidated in `core.py`, ensuring MCP, REST, and agent all share one codebase.

âœ… **Temporal & Semantic Hybrid Ranking**

Combines cosine similarity with a lightweight temporal decay model â€” newer memories surface first.

```python
  score = sim * (1 + Î± * freshness(days))
  ```

  â†’ newer pages rank higher.

âœ… **Dual-Mode Memory**

Supports **short-term** (RAM) and **long-term** (FAISS) memory separation â€” enabling hybrid reasoning loops.

âœ… **Dual Transport (MCP + REST)**

Works both as a traditional MCP stdio toolset *and* a REST API â€” bridging AI agent ecosystems and browser extensions.

âœ… **Dynamic Embedding Backend**

Can seamlessly switch between local (`ollama nomic-embed-text`) and cloud (`text-embedding-004`) without touching code.

âœ… **Agentic Reasoning with Gemini**

Perception and decision stages leverage Gemini 2.0 Flash for contextual tool planning, not static prompts.

âœ… **Data Efficiency**

Uses deduplicated SHA1 chunk hashing and JSONL metadata for minimal storage overhead.

âœ… **Practical RAG Evolution**

Instead of ephemeral chat memory, this agent builds a persistent semantic map of what the user reads online.

---

## ðŸŒ Vision

This project demonstrates how **RAG can evolve into long-term memory**:
an AI system that learns continuously, remembers semantically, and retrieves with context awareness â€” bridging *information retrieval*, *memory persistence*, and *agentic cognition*.

---

## ðŸª„ Summary

> **RAG with Memory** isnâ€™t just another retrieval project â€”
> itâ€™s a **cognitive infrastructure** for AI systems that think, remember, and act autonomously.
> Combining **Gemini reasoning**, **FAISS retrieval**, and **Chrome extension integration**,
> it sets the groundwork for *persistent, context-aware agents*.